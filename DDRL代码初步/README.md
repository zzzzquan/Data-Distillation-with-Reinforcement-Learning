# 强化学习数据蒸馏 (DDRL) 实现

本项目实现了基于强化学习的数据蒸馏算法，通过策略网络智能地从大数据集中选择最有价值的样本子集来训练学生网络。

## 项目结构

- `DDRL01.py`: 完整版实现，使用完整的CIFAR-10数据集进行训练
- `DDRL01_simple.py`: 简化版实现，使用小数据集进行快速测试
- `requirements.txt`: 项目依赖列表

## 核心概念

- **环境 (Environment)**: 使用蒸馏出的数据子集训练学生网络，并在验证集上进行评估的全过程
- **智能体 (Agent)**: 策略网络，负责根据当前状态选择要蒸馏的数据
- **状态 (St)**: 从原始的大训练集中随机采样的一个数据批次
- **动作 (at)**: 策略网络从状态中挑选出的、用于训练的蒸馏数据子集
- **奖励 (Rt)**: 学生网络在使用动作训练后，在固定的验证集上达到的准确率

## 实现步骤

1. **获取状态**: 从原始训练数据集中采样一个较大的批次作为当前状态
2. **生成动作**: 策略网络接收状态作为输入，为每个样本输出选择概率，通过采样决定蒸馏数据子集
3. **训练学生网络**: 每次迭代都重新初始化学生网络，使用蒸馏数据批次进行固定步数训练
4. **计算奖励**: 在固定验证集上评估学生网络性能，得到准确率作为奖励
5. **更新策略网络**: 使用REINFORCE算法更新策略网络参数
6. **重复循环**: 将以上步骤置于主训练循环中运行指定轮数

## 技术规格

- **框架**: PyTorch
- **数据集**: CIFAR-10
- **模型架构**:
  - StudentNetwork: 简单的卷积神经网络
  - PolicyNetwork: CNN结构，为每个样本输出选择概率

## 使用方法

1. 安装依赖:
   ```
   pip install -r requirements.txt
   ```

2. 运行简化版代码（快速测试）:
   ```
   python DDRL01_simple.py
   ```

3. 运行完整版代码:
   ```
   python DDRL01.py
   ```

## 超参数说明

- `STATE_BATCH_SIZE`: 状态批次大小
- `K_STEPS`: 学生网络训练步数
- `NUM_EPISODES`: 总训练轮数
- `LEARNING_RATE_POLICY`: 策略网络学习率
- `LEARNING_RATE_STUDENT`: 学生网络学习率
- `DISTILLATION_RATIO`: 蒸馏比例

## 代码特点

- 实现了完整的REINFORCE强化学习算法
- 包含熵正则化项以鼓励探索
- 使用梯度裁剪防止梯度爆炸
- 添加了详细的日志记录和进度监控
- 提供了简化版代码用于快速测试和验证

## 注意事项

- 训练过程可能需要较长时间，特别是在CPU上运行
- 可以通过调整超参数来平衡训练速度和效果
- 代码支持GPU加速，如果可用会自动使用CUDA